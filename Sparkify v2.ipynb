{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, avg, desc,countDistinct, count, when, concat, lit\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.json(\"mini_sparkify_event_data.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286500, 18)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns Present in data set in the dataframe['artist', 'auth', 'firstName', 'gender', 'itemInSession', 'lastName', 'length', 'level', 'location', 'method', 'page', 'registration', 'sessionId', 'song', 'status', 'ts', 'userAgent', 'userId']\n"
     ]
    }
   ],
   "source": [
    "print(\"columns Present in data set in the dataframe{}\".format(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>228108</td>\n",
       "      <td>286500</td>\n",
       "      <td>278154</td>\n",
       "      <td>278154</td>\n",
       "      <td>286500</td>\n",
       "      <td>278154</td>\n",
       "      <td>228108</td>\n",
       "      <td>286500</td>\n",
       "      <td>278154</td>\n",
       "      <td>286500</td>\n",
       "      <td>286500</td>\n",
       "      <td>278154</td>\n",
       "      <td>286500</td>\n",
       "      <td>228108</td>\n",
       "      <td>286500</td>\n",
       "      <td>286500</td>\n",
       "      <td>278154</td>\n",
       "      <td>286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>551.0852017937219</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>114.41421291448516</td>\n",
       "      <td>None</td>\n",
       "      <td>249.1171819778458</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.5353588340844272E12</td>\n",
       "      <td>1041.526554973822</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>210.05459685863875</td>\n",
       "      <td>1.5409568898104834E12</td>\n",
       "      <td>None</td>\n",
       "      <td>59682.02278593872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1217.7693079161374</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>129.76726201140994</td>\n",
       "      <td>None</td>\n",
       "      <td>99.23517921058361</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.291321616327586E9</td>\n",
       "      <td>726.7762634630741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.50507848842214</td>\n",
       "      <td>1.5075439608226302E9</td>\n",
       "      <td>None</td>\n",
       "      <td>109091.9499991047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>!!!</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Adelaida</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>Adams</td>\n",
       "      <td>0.78322</td>\n",
       "      <td>free</td>\n",
       "      <td>Albany, OR</td>\n",
       "      <td>GET</td>\n",
       "      <td>About</td>\n",
       "      <td>1521380675000</td>\n",
       "      <td>1</td>\n",
       "      <td>ÃÂg ÃÂtti GrÃÂ¡a ÃÂsku</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352117000</td>\n",
       "      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10)...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>ÃÂlafur Arnalds</td>\n",
       "      <td>Logged Out</td>\n",
       "      <td>Zyonna</td>\n",
       "      <td>M</td>\n",
       "      <td>1321</td>\n",
       "      <td>Wright</td>\n",
       "      <td>3024.66567</td>\n",
       "      <td>paid</td>\n",
       "      <td>Winston-Salem, NC</td>\n",
       "      <td>PUT</td>\n",
       "      <td>Upgrade</td>\n",
       "      <td>1543247354000</td>\n",
       "      <td>2474</td>\n",
       "      <td>ÃÂau hafa sloppiÃÂ° undan ÃÂ¾unga myrkursins</td>\n",
       "      <td>404</td>\n",
       "      <td>1543799476000</td>\n",
       "      <td>Mozilla/5.0 (compatible; MSIE 9.0; Windows NT ...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary              artist        auth firstName  gender  \\\n",
       "0   count              228108      286500    278154  278154   \n",
       "1    mean   551.0852017937219        None      None    None   \n",
       "2  stddev  1217.7693079161374        None      None    None   \n",
       "3     min                 !!!   Cancelled  Adelaida       F   \n",
       "4     max   ÃÂlafur Arnalds  Logged Out    Zyonna       M   \n",
       "\n",
       "        itemInSession lastName             length   level           location  \\\n",
       "0              286500   278154             228108  286500             278154   \n",
       "1  114.41421291448516     None  249.1171819778458    None               None   \n",
       "2  129.76726201140994     None  99.23517921058361    None               None   \n",
       "3                   0    Adams            0.78322    free         Albany, OR   \n",
       "4                1321   Wright         3024.66567    paid  Winston-Salem, NC   \n",
       "\n",
       "   method     page           registration          sessionId  \\\n",
       "0  286500   286500                 278154             286500   \n",
       "1    None     None  1.5353588340844272E12  1041.526554973822   \n",
       "2    None     None    3.291321616327586E9  726.7762634630741   \n",
       "3     GET    About          1521380675000                  1   \n",
       "4     PUT  Upgrade          1543247354000               2474   \n",
       "\n",
       "                                               song              status  \\\n",
       "0                                            228108              286500   \n",
       "1                                          Infinity  210.05459685863875   \n",
       "2                                               NaN   31.50507848842214   \n",
       "3                    \n",
       "ÃÂg ÃÂtti GrÃÂ¡a ÃÂsku                 200   \n",
       "4  ÃÂau hafa sloppiÃÂ° undan ÃÂ¾unga myrkursins                 404   \n",
       "\n",
       "                      ts                                          userAgent  \\\n",
       "0                 286500                                             278154   \n",
       "1  1.5409568898104834E12                                               None   \n",
       "2   1.5075439608226302E9                                               None   \n",
       "3          1538352117000  \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10)...   \n",
       "4          1543799476000  Mozilla/5.0 (compatible; MSIE 9.0; Windows NT ...   \n",
       "\n",
       "              userId  \n",
       "0             286500  \n",
       "1  59682.02278593872  \n",
       "2  109091.9499991047  \n",
       "3                     \n",
       "4                 99  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>228108</td>\n",
       "      <td>286500</td>\n",
       "      <td>278154</td>\n",
       "      <td>278154</td>\n",
       "      <td>286500</td>\n",
       "      <td>278154</td>\n",
       "      <td>228108</td>\n",
       "      <td>286500</td>\n",
       "      <td>278154</td>\n",
       "      <td>286500</td>\n",
       "      <td>286500</td>\n",
       "      <td>278154</td>\n",
       "      <td>286500</td>\n",
       "      <td>228108</td>\n",
       "      <td>286500</td>\n",
       "      <td>286500</td>\n",
       "      <td>278154</td>\n",
       "      <td>286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>551.0852017937219</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>114.41421291448516</td>\n",
       "      <td>None</td>\n",
       "      <td>249.1171819778458</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.5353588340844272E12</td>\n",
       "      <td>1041.526554973822</td>\n",
       "      <td>Infinity</td>\n",
       "      <td>210.05459685863875</td>\n",
       "      <td>1.5409568898104834E12</td>\n",
       "      <td>None</td>\n",
       "      <td>59682.02278593872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1217.7693079161374</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>129.76726201140994</td>\n",
       "      <td>None</td>\n",
       "      <td>99.23517921058361</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.291321616327586E9</td>\n",
       "      <td>726.7762634630741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.50507848842214</td>\n",
       "      <td>1.5075439608226302E9</td>\n",
       "      <td>None</td>\n",
       "      <td>109091.9499991047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>!!!</td>\n",
       "      <td>Cancelled</td>\n",
       "      <td>Adelaida</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>Adams</td>\n",
       "      <td>0.78322</td>\n",
       "      <td>free</td>\n",
       "      <td>Albany, OR</td>\n",
       "      <td>GET</td>\n",
       "      <td>About</td>\n",
       "      <td>1521380675000</td>\n",
       "      <td>1</td>\n",
       "      <td>ÃÂg ÃÂtti GrÃÂ¡a ÃÂsku</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352117000</td>\n",
       "      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10)...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>ÃÂlafur Arnalds</td>\n",
       "      <td>Logged Out</td>\n",
       "      <td>Zyonna</td>\n",
       "      <td>M</td>\n",
       "      <td>1321</td>\n",
       "      <td>Wright</td>\n",
       "      <td>3024.66567</td>\n",
       "      <td>paid</td>\n",
       "      <td>Winston-Salem, NC</td>\n",
       "      <td>PUT</td>\n",
       "      <td>Upgrade</td>\n",
       "      <td>1543247354000</td>\n",
       "      <td>2474</td>\n",
       "      <td>ÃÂau hafa sloppiÃÂ° undan ÃÂ¾unga myrkursins</td>\n",
       "      <td>404</td>\n",
       "      <td>1543799476000</td>\n",
       "      <td>Mozilla/5.0 (compatible; MSIE 9.0; Windows NT ...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary              artist        auth firstName  gender  \\\n",
       "0   count              228108      286500    278154  278154   \n",
       "1    mean   551.0852017937219        None      None    None   \n",
       "2  stddev  1217.7693079161374        None      None    None   \n",
       "3     min                 !!!   Cancelled  Adelaida       F   \n",
       "4     max   ÃÂlafur Arnalds  Logged Out    Zyonna       M   \n",
       "\n",
       "        itemInSession lastName             length   level           location  \\\n",
       "0              286500   278154             228108  286500             278154   \n",
       "1  114.41421291448516     None  249.1171819778458    None               None   \n",
       "2  129.76726201140994     None  99.23517921058361    None               None   \n",
       "3                   0    Adams            0.78322    free         Albany, OR   \n",
       "4                1321   Wright         3024.66567    paid  Winston-Salem, NC   \n",
       "\n",
       "   method     page           registration          sessionId  \\\n",
       "0  286500   286500                 278154             286500   \n",
       "1    None     None  1.5353588340844272E12  1041.526554973822   \n",
       "2    None     None    3.291321616327586E9  726.7762634630741   \n",
       "3     GET    About          1521380675000                  1   \n",
       "4     PUT  Upgrade          1543247354000               2474   \n",
       "\n",
       "                                               song              status  \\\n",
       "0                                            228108              286500   \n",
       "1                                          Infinity  210.05459685863875   \n",
       "2                                               NaN   31.50507848842214   \n",
       "3                    \n",
       "ÃÂg ÃÂtti GrÃÂ¡a ÃÂsku                 200   \n",
       "4  ÃÂau hafa sloppiÃÂ° undan ÃÂ¾unga myrkursins                 404   \n",
       "\n",
       "                      ts                                          userAgent  \\\n",
       "0                 286500                                             278154   \n",
       "1  1.5409568898104834E12                                               None   \n",
       "2   1.5075439608226302E9                                               None   \n",
       "3          1538352117000  \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10)...   \n",
       "4          1543799476000  Mozilla/5.0 (compatible; MSIE 9.0; Windows NT ...   \n",
       "\n",
       "              userId  \n",
       "0             286500  \n",
       "1  59682.02278593872  \n",
       "2  109091.9499991047  \n",
       "3                     \n",
       "4                 99  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lets look at schema of data :\n",
    "df.printSchema();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NAs there is no null values in columns in userId and sessionId\n",
    "df = df.dropna(how = 'any', subset = ['userId', 'sessionId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop empty strings\n",
    "df = df.filter(df['userId'] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.csv('test_20191218.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278154, 18)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                page|count(page)|\n",
      "+--------------------+-----------+\n",
      "|              Cancel|         52|\n",
      "|    Submit Downgrade|         63|\n",
      "|         Thumbs Down|       2546|\n",
      "|                Home|      10082|\n",
      "|           Downgrade|       2055|\n",
      "|         Roll Advert|       3933|\n",
      "|              Logout|       3226|\n",
      "|       Save Settings|        310|\n",
      "|Cancellation Conf...|         52|\n",
      "|               About|        495|\n",
      "|            Settings|       1514|\n",
      "|     Add to Playlist|       6526|\n",
      "|          Add Friend|       4277|\n",
      "|            NextSong|     228108|\n",
      "|           Thumbs Up|      12551|\n",
      "|                Help|       1454|\n",
      "|             Upgrade|        499|\n",
      "|               Error|        252|\n",
      "|      Submit Upgrade|        159|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('page','UserId').groupby('page').agg({'page':'count'}).select('page','count(page)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select unique users with cancellation confirmation in the page column \n",
    "churned_user_ids = df.filter(df.page == 'Cancellation Confirmation')\\\n",
    "                                            .select('userId')\\\n",
    "                                            .dropDuplicates()\\\n",
    "                                            .rdd.flatMap(lambda x : x)\\\n",
    "                                            .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define churn with 1 being users who have pressed “cancellation confirmation”, 0 otherwise\n",
    "df = df.withColumn('churn', when(col(\"userId\").isin(churned_user_ids), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a temp view from dataframe df,so that we can use sparkSQL for quick easy analysis\n",
    "df.createOrReplaceTempView('data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparkSQL to understand churn rate\n",
    "df_churn = spark.sql(\"\"\"\n",
    "              SELECT\n",
    "                  churn,\n",
    "                  count(distinct userId) as unique_user_count\n",
    "                FROM\n",
    "                    data\n",
    "                GROUP BY\n",
    "                    churn\n",
    "                \"\"\")\n",
    "df_churn.show()\n",
    "\n",
    "n_users_churn = df_churn.filter(col('churn') == '1')\\\n",
    "                             .select('unique_user_count').first()[0]\n",
    "n_users_remain = df_churn.filter(col('churn') == '0')\\\n",
    "                             .select('unique_user_count').first()[0]\n",
    "\n",
    "print('The number of churn user: {} with a churn rate of: {}'\\\n",
    "                             .format(n_users_churn,round((n_users_churn/(n_users_churn+n_users_remain)),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, it seems that 'Cancelled' in 'auth' column also represents churn. let's just try to understand the data a bit more...\n",
    "df_auth = spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                auth, \n",
    "                count(distinct userId) as unique_user_count\n",
    "            FROM \n",
    "                data\n",
    "            WHERE \n",
    "                userId IS NOT NULL AND userID != ''\n",
    "            GROUP BY \n",
    "                auth\n",
    "            ORDER BY \n",
    "                unique_user_count DESC\n",
    "            \"\"\")\n",
    "\n",
    "df_auth.show()\n",
    "\n",
    "n_users_registered = df_auth.filter(col('auth') == 'Logged In')\\\n",
    "                             .select('unique_user_count').first()[0]\n",
    "n_users_cancelled = df_auth.filter(col('auth') == 'Cancelled')\\\n",
    "                             .select('unique_user_count').first()[0]\n",
    "\n",
    "print('The number of users registered: {} and the number of users left: {} with a churn rate of: {}'\\\n",
    "                             .format(n_users_registered,n_users_cancelled,round((n_users_cancelled/n_users_registered),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the average number of songs played by churned and unchurned user.\n",
    "df_songs_count = df.filter(col('page')=='NextSong')\\\n",
    "                                .groupby(col('churn'))\\\n",
    "                                .count()\n",
    "\n",
    "df_songs_unique_users = df.filter(col('page')=='NextSong')\\\n",
    "                                .groupby(col('churn'))\\\n",
    "                                .agg(countDistinct(col('userId')))\n",
    "\n",
    "avg_num_songs_played = df_songs_unique_users\\\n",
    "                .join(df_songs_count, df_songs_count.churn == df_songs_unique_users.churn)\\\n",
    "                .drop(df_songs_count.churn)\n",
    "\n",
    "avg_num_songs_played = avg_num_songs_played\\\n",
    "                .withColumn('AvgNumberofSongs', \\\n",
    "                avg_num_songs_played['count']/avg_num_songs_played['count(DISTINCT userId)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting average number of songs for churn and non-churn users\n",
    "df_plt = avg_num_songs_played.select(['churn', 'AvgNumberofSongs']).toPandas()\n",
    "sns.barplot(x=\"churn\", y=\"AvgNumberofSongs\", data = df_plt)\n",
    "plt.xlabel(\"churn\", fontsize=14);\n",
    "plt.ylabel(\"Average Number of Songs Played\", fontsize=12);\n",
    "plt.title(\"Average Number of songs played:\\nchurned users vs unchurned users\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"churn\", y=\"AvgNumberofSongs\", data = df_plt)\n",
    "plt.xlabel(\"churn\", fontsize=14);\n",
    "plt.ylabel(\"Average Number of Songs Played\", fontsize=12);\n",
    "plt.title(\"Average Number of songs played:\\nchurned users vs unchurned users\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of times each page is vistied: churned vs unchurned users.\n",
    "df_page_churn_count = df.groupby(['churn','page']).agg({'page':'count'})\n",
    "\n",
    "avg_num_page_visit = df_page_churn_count\\\n",
    "                .join(df_songs_unique_users, df_songs_unique_users.churn == df_page_churn_count.churn)\\\n",
    "                .drop(df_songs_unique_users.churn)\n",
    "\n",
    "avg_num_page_visit = avg_num_page_visit\\\n",
    "                .withColumn('AvgNumberofPageVisit', \\\n",
    "                            avg_num_page_visit['count(page)']/avg_num_page_visit['count(DISTINCT userId)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am leaving out \"NextSong\" page as it dominates every other page, Moreover NextSong has been shown seperately in the plot above\n",
    "avg_num_page_visit = avg_num_page_visit.filter(col('page') != 'NextSong')\n",
    "avg_num_page_visit = avg_num_page_visit.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting average number of times each page is vistied: churned vs unchurned users.\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "sns.barplot(x=\"page\",y=\"AvgNumberofPageVisit\", hue=\"churn\", data=avg_num_page_visit, hue_order=[1, 0]);\n",
    "plt.xticks(rotation=90, fontsize=14);\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(\"page\", fontsize=14)\n",
    "plt.ylabel(\"Average number of page visits\", fontsize=14)\n",
    "plt.title(\"Average number of page visits: churned users vs unchurned users\", fontsize=14)\n",
    "plt.xticks(rotation=30, ha='right', fontsize=14)\n",
    "plt.legend(loc='best', fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of users churned while being free subsribers vs paid subscribers\n",
    "df_pd = df.filter(col('churn')==1).filter(df['page']==\"Cancellation Confirmation\").groupby(\"level\").count().toPandas()\n",
    "sns.barplot(x=\"level\", y=\"count\", data=df_pd);\n",
    "plt.xlabel(\"level\", fontsize=14);\n",
    "plt.ylabel(\"Number of users churned\", fontsize=14);\n",
    "plt.title(\"Number of users churned while being: free subsribers vs paid subscribers\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking ratio of male and female in churn and no churn user\n",
    "df_gen = df.dropDuplicates([\"userId\", \"gender\"]).groupby([\"churn\", \"gender\"]).count().sort(\"churn\").toPandas()\n",
    "sns.barplot(x='churn', y='count', hue='gender', data=df_gen)\n",
    "plt.title(\"ratio of male female in churn and no churn user\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing all the column above i have decided to use below features in my model: <br/>\n",
    " 1:- Gender\n",
    " 2:- UserAgent\n",
    " 3:- Status\n",
    " 4;- Page\n",
    " 5:- Level\n",
    " 6:- ItemInSession\n",
    "\n",
    "Once the columns were identified, we now have to make sure that they are all in the numeric datatype so that they could be put into the model that we choose. \n",
    "The Gender, UserAgent, level and page columns had to be converted into numeric values using a combination of String Indexing and One Hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build pipeline\n",
    "Gender_indexer = StringIndexer(inputCol=\"gender\", outputCol='Gender_Index')\n",
    "User_indexer = StringIndexer(inputCol=\"userAgent\", outputCol='User_Index')\n",
    "Page_indexer = StringIndexer(inputCol=\"page\", outputCol='Page_Index')\n",
    "Level_Indexer = StringIndexer(inputCol=\"level\", outputCol='Level_Index')\n",
    "\n",
    "Gender_encoder = OneHotEncoder(inputCol='Gender_Index', outputCol='Gender_Vec')\n",
    "User_encoder = OneHotEncoder(inputCol='User_Index', outputCol='User_Vec')\n",
    "Page_encoder = OneHotEncoder(inputCol='Page_Index', outputCol='Page_Vec')\n",
    "Level_encoder = OneHotEncoder(inputCol='Level_Index', outputCol='Level_Vec')\n",
    "\n",
    "#create VectorAssembler to push data to ML models\n",
    "assembler = VectorAssembler(inputCols=[\"Gender_Vec\", \"User_Vec\", \"Page_Vec\",\"Level_Vec\", \"itemInSession\",\"status\"], outputCol=\"features\")\n",
    "indexer = StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
    "\n",
    "#Lets normalize data\n",
    "scaler = Normalizer(inputCol=\"features\", outputCol=\"ScaledFeatures\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select classification functions\n",
    "lr =  LogisticRegression(featuresCol=\"ScaledFeatures\", labelCol = \"label\", maxIter=10, regParam=0.01, elasticNetParam=0)\n",
    "rf = RandomForestClassifier(labelCol=\"label\",featuresCol=\"ScaledFeatures\", featureSubsetStrategy='sqrt')\n",
    "gbt = GBTClassifier(featuresCol=\"ScaledFeatures\", labelCol=\"label\",featureSubsetStrategy='sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Pipelines\n",
    "pipeline_lr = Pipeline(stages=[Gender_indexer, User_indexer, Page_indexer, Level_Indexer, Gender_encoder,\\\n",
    "                            User_encoder, Page_encoder, Level_encoder, assembler, indexer, scaler,lr])\n",
    "\n",
    "pipeline_rf = Pipeline(stages=[Gender_indexer, User_indexer, Page_indexer, Level_Indexer, Gender_encoder,\n",
    "                            User_encoder, Page_encoder, Level_encoder, assembler, indexer, scaler, rf])\n",
    "\n",
    "pipeline_gbt = Pipeline(stages=[Gender_indexer, User_indexer, Page_indexer, Level_Indexer, Gender_encoder,\n",
    "                            User_encoder, Page_encoder, Level_encoder, assembler, indexer, scaler, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split: As a first step break your data set into 90% \n",
    "#of training data and set aside 10%. Set random seed to 42.\n",
    "train, test = df.randomSplit([0.9, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a base model before parameter turning\n",
    "\n",
    "model_base_lr = pipeline_lr.fit(train)\n",
    "model_base_rf = pipeline_rf.fit(train)\n",
    "model_base_gbt = pipeline_gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function will calcualte f1 scores:\n",
    "def model_performance(model, test_data, metric = 'f1'):\n",
    "    \"\"\" Calculate Model Scores using f1 metric \n",
    "        Input: \n",
    "            model- trained model or pipeline object\n",
    "            metric- the metric used to measure performance\n",
    "            data - data on which performance measurement should be done\n",
    "        Output:\n",
    "            score\n",
    "    \"\"\"\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName = metric)\n",
    "    prediction_result = model.transform(test_data)\n",
    "    # find f1 score\n",
    "    score = evaluator.evaluate(prediction_result)\n",
    "    #return score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_base_lr = model_performance (model_base_lr, test)\n",
    "F1_base_rf = model_performance (model_base_rf, test)\n",
    "F1_base_gbt = model_performance (model_base_gbt, test)\n",
    "\n",
    "print(\"Logistic Regression Base Model F1:{}\".format(round(F1_base_lr,4)));\n",
    "print(\"Random Forest Base Model F1:{}\".format(round(F1_base_rf,4)));\n",
    "print(\"GBT Classifier Base Model F1:{}\".format(round(F1_base_gbt,4)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistics Regression Parameter Turning\n",
    "import time\n",
    "start_time= time.time()\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam,[0.0, 0.1,]) \\\n",
    "    .build()\n",
    "\n",
    "cv_lr = CrossValidator(estimator=pipeline_lr,\n",
    "                          estimatorParamMaps=paramGrid_lr,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel_lr = cv_lr.fit(train)\n",
    "\n",
    "end_time= time.time()\n",
    "\n",
    "print(\"Total execution time for logistic:{}\".format(round(end_time-start_time),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests model Parameter Turning\n",
    "import time\n",
    "start_time= time.time()\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.impurity,['entropy', 'gini']) \\\n",
    "    .addGrid(rf.maxDepth,[2, 4, 6]) \\\n",
    "    .addGrid(rf.numTrees,[10, 25, 50]) \\\n",
    "    .build()\n",
    "\n",
    "cv_rf = CrossValidator(estimator=pipeline_rf,\n",
    "                             estimatorParamMaps=paramGrid_rf,\n",
    "                             evaluator=MulticlassClassificationEvaluator(),\n",
    "                             numFolds=3)\n",
    "\n",
    "cvModel_rf = cv_rf.fit(train)\n",
    "cvModel_rf.transform(test)\n",
    "avgMetrics_rf = cvModel_rf.avgMetrics\n",
    "\n",
    "\n",
    "end_time= time.time()\n",
    "print(\"Total execution time for random forest model is:{}\".format(round(end_time-start_time),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBT model Parameter Turning\n",
    "import time\n",
    "start_time= time.time()\n",
    "\n",
    "paramGrid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth,[4,6]) \\\n",
    "    .addGrid(gbt.stepSize,[0.01, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "cv_gbt = CrossValidator(estimator=pipeline_gbt,\n",
    "                             estimatorParamMaps=paramGrid_gbt,\n",
    "                             evaluator=MulticlassClassificationEvaluator(),\n",
    "                             numFolds=3)\n",
    "\n",
    "cvModel_gbt = cv_gbt.fit(train)\n",
    "cvModel_gbt.transform(test)\n",
    "avgMetrics_gbt = cvModel_gbt.avgMetrics\n",
    "print (avgMetrics_gbt)\n",
    "end_time= time.time()\n",
    "print(\"Total execution time for random forest model is:{}\".format(round(end_time-start_time),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return model performance for the improved models\n",
    "F1_lr = model_performance(cvModel_lr, test)\n",
    "F1_rf = model_performance(cvModel_rf, test)\n",
    "F1_gbt = model_performance(cvModel_gbt, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print Model Testing F1-Score\n",
    "print(\"Logistic Regression Improved Model F1:{}\".format(round(F1_lr,4)));\n",
    "print(\"Random Forest Classifier Improved Model F1:{}\".format(round(F1_rf,4)));\n",
    "print(\"GBT Classifier Improved Model F1:{}\".format(round(F1_gbt,4)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function will calcualte model accuracy:\n",
    "def model_performance(model, test_data, metric = 'accuracy'):\n",
    "    \"\"\" Calculate Model Scores using Accuracy Score \n",
    "        Input: \n",
    "            model- trained model or pipeline object\n",
    "            metric- the metric used to measure performance\n",
    "            data - data on which performance measurement should be done\n",
    "        Output:\n",
    "            score\n",
    "    \"\"\"\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName = metric)\n",
    "    prediction_result = model.transform(test_data)\n",
    "    # find f1 score\n",
    "    score = evaluator.evaluate(prediction_result)\n",
    "    #return score\n",
    "    return score\n",
    "\n",
    "Accuracy_lr = model_performance(cvModel_lr, test)\n",
    "Accuracy_rf = model_performance(cvModel_rf, test)\n",
    "Accuracy_gbt = model_performance(cvModel_gbt, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print Model Testing Accuracy Score\n",
    "print(\"Logistic Regression Model Accuracy:{}\".format(round(Accuracy_lr,4)));\n",
    "print(\"Random Forest Classifier Accuracy:{}\".format(round(Accuracy_rf,4)));\n",
    "print(\"GBT Classifier Accuracy:{}\".format(round(Accuracy_gbt,4)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a F1 score of 0.874 and accuracy of 0.8948, GBTClassifier is a preferred model to perform best with the sample data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have the best parameters used in a different dataset, let us return the result of Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GBT Model: maxDepth=6\n",
      "Best GBT Model: param stepSize=0.1\n"
     ]
    }
   ],
   "source": [
    "#Parameter Selection\n",
    "bestPipeline = cvModel_gbt.bestModel\n",
    "bestGBTModel = bestPipeline.stages[-1]\n",
    "#bestParams = bestGBTModel.extractParamMap()\n",
    "#bestParams\n",
    "print('Best GBT Model: maxDepth={}'.format(bestGBTModel._java_obj.getMaxDepth()))\n",
    "print('Best GBT Model: param stepSize={}'.format(bestGBTModel._java_obj.getStepSize()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Result on a medium size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martin Orford', auth='Logged In', firstName='Joseph', gender='M', itemInSession=20, lastName='Morales', length=597.55057, level='free', location='Corpus Christi, TX', method='PUT', page='NextSong', registration=1532063507000, sessionId=292, song='Grand Designs', status=200, ts=1538352011000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='293')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = spark.read.json(\"medium-sparkify-event-data.json\")\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(543705, 18)\n"
     ]
    }
   ],
   "source": [
    "print((df_new.count(), len(df_new.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Processing on the medium size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select unique users with cancellation confirmation in the page column \n",
    "churned_user_ids = df_new.filter(df_new.page == 'Cancellation Confirmation')\\\n",
    "                                            .select('userId')\\\n",
    "                                            .dropDuplicates()\\\n",
    "                                            .rdd.flatMap(lambda x : x)\\\n",
    "                                            .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define churn with 1 being users who have pressed “cancellation confirmation”, 0 otherwise\n",
    "df_new = df_new.withColumn('churn', when(col(\"userId\").isin(churned_user_ids), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.randomSplit([0.9, 0.1], seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
